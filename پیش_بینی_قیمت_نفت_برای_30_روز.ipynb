{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1yvy_Cve50BQtCwQEWuNJ5VHEb6mEvjIa",
      "authorship_tag": "ABX9TyPGM3ud2TTT7o6uJqOBang9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zabihullahburhani/-Improving-the-Accuracy-of-Crude-Oil-Price-Prediction-Using-a-Transformer-Based-Hybrid-Approach/blob/main/%D9%BE%DB%8C%D8%B4_%D8%A8%DB%8C%D9%86%DB%8C_%D9%82%DB%8C%D9%85%D8%AA_%D9%86%D9%81%D8%AA_%D8%A8%D8%B1%D8%A7%DB%8C_30_%D8%B1%D9%88%D8%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i2OwzJ7DAbj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('Crude oil.csv', parse_dates=['Date'], dayfirst=True)\n",
        "data.set_index('Date', inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"Crude Oil Price Prediction: An Advanced Data-Driven Approach Utilizing Artificial Intelligence and Machine Learning Techniques for Enhanced Forecasting Accuracy\"\n",
        "\n",
        "\n",
        "\"پیش‌بینی قیمت نفت خام: یک رویکرد پیشرفته داده‌محور با استفاده از تکنیک‌های هوش مصنوعی و یادگیری ماشین برای افزایش دقت پیش‌بینی‌ها\""
      ],
      "metadata": {
        "id": "Rbl6A1LXyudh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and fill missing values\n",
        "data = data.fillna(method='ffill')\n"
      ],
      "metadata": {
        "id": "VJA3BIFFDIW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Function to visualize the distribution and identify outliers\n",
        "def analyze_data_distribution(data):\n",
        "    # Ensure that data is a Pandas DataFrame (if not, convert it)\n",
        "    if isinstance(data, np.ndarray):\n",
        "        data = pd.DataFrame(data, columns=['Close'])\n",
        "\n",
        "    # Visualize the distribution of the data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data['Close'], kde=True, bins=30, color='skyblue')\n",
        "    plt.title(\"Distribution of Oil Price Data\")\n",
        "    plt.xlabel(\"Price\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Display basic statistics (mean, median, std)\n",
        "    print(\"Basic Statistics:\")\n",
        "    print(data['Close'].describe())\n",
        "\n",
        "    # Count values less than 20 and negative values\n",
        "    negative_values = len(data[data['Close'] < 0])\n",
        "    low_values = len(data[data['Close'] < 20])\n",
        "    print(f\"\\nNumber of negative values: {negative_values}\")\n",
        "    print(f\"Number of values less than 20: {low_values}\")\n",
        "\n",
        "    # You can also return the identified outliers for further inspection\n",
        "    negative_values_data = data[data['Close'] < 0]\n",
        "    low_values_data = data[data['Close'] < 20]\n",
        "\n",
        "    return negative_values_data, low_values_data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "negative_values, low_values = analyze_data_distribution(data)\n",
        "\n",
        "# Display the identified negative and low-value records\n",
        "print(f\"\\nNegative values:\\n{negative_values.head()}\")\n",
        "print(f\"\\nValues less than 20:\\n{low_values.head()}\")\n"
      ],
      "metadata": {
        "id": "bBHvm6zSF3sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean the oil price data\n",
        "def clean_oil_data(data):\n",
        "    # Ensure that data is a Pandas DataFrame (if not, convert it)\n",
        "    if isinstance(data, np.ndarray):\n",
        "        data = pd.DataFrame(data, columns=['Close'])\n",
        "\n",
        "    # Remove negative values and values between 0 and 20\n",
        "    cleaned_data = data[(data['Close'] > 15) & (data['Close'] >= 0)]  # Only keep valid prices\n",
        "    return cleaned_data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_cleaned = clean_oil_data(data)  # Replace 'data' with your dataset\n",
        "print(f\"Cleaned Data: {data_cleaned.head()}\")  # Preview the cleaned data\n"
      ],
      "metadata": {
        "id": "kj0jHfQcF34j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "data = data_cleaned\n",
        "# Function to visualize the distribution and identify outliers\n",
        "def analyze_data_distribution(data):\n",
        "    # Ensure that data is a Pandas DataFrame (if not, convert it)\n",
        "    if isinstance(data, np.ndarray):\n",
        "        data = pd.DataFrame(data, columns=['Close'])\n",
        "\n",
        "    # Visualize the distribution of the data\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data['Close'], kde=True, bins=30, color='skyblue')\n",
        "    plt.title(\"Distribution of Oil Price Data\")\n",
        "    plt.xlabel(\"Price\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Display basic statistics (mean, median, std)\n",
        "    print(\"Basic Statistics:\")\n",
        "    print(data['Close'].describe())\n",
        "\n",
        "    # Count values less than 20 and negative values\n",
        "    negative_values = len(data[data['Close'] < 0])\n",
        "    low_values = len(data[data['Close'] < 20])\n",
        "    print(f\"\\nNumber of negative values: {negative_values}\")\n",
        "    print(f\"Number of values less than 20: {low_values}\")\n",
        "\n",
        "    # You can also return the identified outliers for further inspection\n",
        "    negative_values_data = data[data['Close'] < 0]\n",
        "    low_values_data = data[data['Close'] < 20]\n",
        "\n",
        "    return negative_values_data, low_values_data\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "negative_values, low_values = analyze_data_distribution(data)\n",
        "\n",
        "# Display the identified negative and low-value records\n",
        "print(f\"\\nNegative values:\\n{negative_values.head()}\")\n",
        "print(f\"\\nValues less than 20:\\n{low_values.head()}\")\n"
      ],
      "metadata": {
        "id": "fEm9X4GXF3e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "mU3OczVQG3Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mn2yj89HO6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to apply ARIMA with Cross Validation\n",
        "def arima_cv_with_barplot(data, order=(1, 1, 1), n_splits=10):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data):\n",
        "        train, test = data[train_index], data[test_index]\n",
        "\n",
        "        # Fit ARIMA Model\n",
        "        model = ARIMA(train, order=order)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Forecast\n",
        "        predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "        mae = mean_absolute_error(test, predictions)\n",
        "        mape = np.mean(np.abs((test - predictions) / test)) * 100\n",
        "        r2 = r2_score(test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"ARIMA Model Evaluation (Order: {order}):\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Barplot of Final Results\n",
        "    metrics = {\n",
        "        'RMSE': avg_rmse,\n",
        "        'MAE': avg_mae,\n",
        "        'MAPE': avg_mape,\n",
        "\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green'], alpha=0.8, edgecolor='black')\n",
        "    plt.title(\"Final Evaluation Metrics\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "metrics = arima_cv_with_barplot(data_close, order=(1, 1, 1))\n"
      ],
      "metadata": {
        "id": "PjTDug4dDITB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to apply ARIMA with Cross Validation and Future Forecasting\n",
        "def arima_cv_with_barplot_and_forecasting(data, order=(1, 1, 1), n_splits=10, future_days=120):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data):\n",
        "        train, test = data[train_index], data[test_index]\n",
        "\n",
        "        # Fit ARIMA Model\n",
        "        model = ARIMA(train, order=order)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Forecast on Test Data\n",
        "        predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "        mae = mean_absolute_error(test, predictions)\n",
        "        mape = np.mean(np.abs((test - predictions) / test)) * 100\n",
        "        r2 = r2_score(test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"ARIMA Model Evaluation (Order: {order}):\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Barplot of Final Results\n",
        "    metrics = {\n",
        "        'RMSE': avg_rmse,\n",
        "        'MAE': avg_mae,\n",
        "        'MAPE': avg_mape,\n",
        "        'R^2': avg_r2\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red'], alpha=0.8, edgecolor='black')\n",
        "    plt.title(\"Final Evaluation Metrics\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # Future Forecasting (30 days ahead)\n",
        "    # Train the model on the full data\n",
        "    model_full = ARIMA(data, order=order)\n",
        "    model_fit_full = model_full.fit()\n",
        "\n",
        "    # Forecast the next 30 days\n",
        "    forecast_30_days = model_fit_full.forecast(steps=future_days)\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.arange(len(data)), data, label='Historical Data', color='blue')\n",
        "    plt.plot(np.arange(len(data), len(data) + future_days), forecast_30_days, label='Future Forecast (120 Days)', color='red')\n",
        "    plt.title(f'30 Days Future Forecast using ARIMA (Order: {order})')\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "metrics = arima_cv_with_barplot_and_forecasting(data_close, order=(1, 1, 1))\n"
      ],
      "metadata": {
        "id": "XkxgnWdzD-VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Function to apply SARIMAX with Cross Validation and future forecasting\n",
        "def sarimax_cv_with_forecast(data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7), n_splits=10, forecast_steps=120):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data):\n",
        "        train, test = data[train_index], data[test_index]\n",
        "\n",
        "        # Fit SARIMAX Model\n",
        "        model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
        "        model_fit = model.fit(disp=False)\n",
        "\n",
        "        # Forecast for the test set\n",
        "        predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "        mae = mean_absolute_error(test, predictions)\n",
        "        mape = np.mean(np.abs((test - predictions) / test)) * 100\n",
        "        r2 = r2_score(test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"SARIMAX Model Evaluation (Order: {order}, Seasonal Order: {seasonal_order}):\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Plot Metrics Barplot\n",
        "    metrics = {\n",
        "        'RMSE': avg_rmse,\n",
        "        'MAE': avg_mae,\n",
        "        'MAPE': avg_mape,\n",
        "        'R^2': avg_r2\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red'], alpha=0.8, edgecolor='black')\n",
        "    plt.title(\"Final Evaluation Metrics (SARIMAX)\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # Forecast next 120 days\n",
        "    model_fit_full = SARIMAX(data, order=order, seasonal_order=seasonal_order)\n",
        "    model_fit_full_result = model_fit_full.fit(disp=False)\n",
        "    forecast = model_fit_full_result.forecast(steps=forecast_steps)\n",
        "\n",
        "    # Plot forecasted values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data, label=\"Original Data\")\n",
        "    plt.plot(np.arange(len(data), len(data) + forecast_steps), forecast, color='red', label=\"Forecast\")\n",
        "    plt.title(f\"120 Day Forecast Using SARIMAX\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics, forecast\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "metrics_sarimax, forecast_120_days = sarimax_cv_with_forecast(data_close, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7), forecast_steps=120)\n"
      ],
      "metadata": {
        "id": "pVIlwSV2FM6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Function to apply Holt-Winters with Cross Validation and future forecasting\n",
        "def holt_winters_cv_with_forecast(data, seasonal_periods=7, trend='add', seasonal='add', n_splits=10, forecast_steps=120):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data):\n",
        "        train, test = data[train_index], data[test_index]\n",
        "\n",
        "        # Fit Holt-Winters Model\n",
        "        model = ExponentialSmoothing(train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        # Forecast for the test set\n",
        "        predictions = model_fit.forecast(steps=len(test))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "        mae = mean_absolute_error(test, predictions)\n",
        "        mape = np.mean(np.abs((test - predictions) / test)) * 100\n",
        "        r2 = r2_score(test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"Holt-Winters Model Evaluation (Seasonal Periods: {seasonal_periods}, Trend: {trend}, Seasonal: {seasonal}):\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Plot Metrics Barplot\n",
        "    metrics = {\n",
        "        'RMSE': avg_rmse,\n",
        "        'MAE': avg_mae,\n",
        "        'MAPE': avg_mape,\n",
        "        'R^2': avg_r2\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red'], alpha=0.8, edgecolor='black')\n",
        "    plt.title(\"Final Evaluation Metrics (Holt-Winters)\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # Forecast next 120 days\n",
        "    model_fit_full = ExponentialSmoothing(data, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
        "    model_fit_full_result = model_fit_full.fit()\n",
        "    forecast = model_fit_full_result.forecast(steps=forecast_steps)\n",
        "\n",
        "    # Plot forecasted values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data, label=\"Original Data\")\n",
        "    plt.plot(np.arange(len(data), len(data) + forecast_steps), forecast, color='red', label=\"Forecast\")\n",
        "    plt.title(f\"120 Day Forecast Using Holt-Winters\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics, forecast\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "metrics_holt_winters, forecast_120_days = holt_winters_cv_with_forecast(data_close, seasonal_periods=7, forecast_steps=120)\n"
      ],
      "metadata": {
        "id": "aMVqRbp1KirX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Function to create sequences for LSTM\n",
        "def create_sequences(data, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:i + look_back])\n",
        "        y.append(data[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to apply LSTM with Cross Validation and future forecasting\n",
        "def lstm_cv_with_forecast(data, look_back=10, n_splits=10, forecast_steps=120, epochs=10, batch_size=32):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data_scaled):\n",
        "        train, test = data_scaled[train_index], data_scaled[test_index]\n",
        "        X_train, y_train = create_sequences(train, look_back)\n",
        "        X_test, y_test = create_sequences(test, look_back)\n",
        "\n",
        "        # Reshape data for LSTM [samples, time steps, features]\n",
        "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "        # Define LSTM Model\n",
        "        model = Sequential([\n",
        "            LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "            LSTM(50, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        # Train Model\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Predict\n",
        "        predictions = model.predict(X_test)\n",
        "        predictions = scaler.inverse_transform(predictions)\n",
        "        y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"LSTM Model Evaluation:\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Plot Metrics Barplot\n",
        "    metrics = {\n",
        "        'RMSE': avg_rmse,\n",
        "        'MAE': avg_mae,\n",
        "        'MAPE': avg_mape,\n",
        "        'R^2': avg_r2\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red'], alpha=0.8, edgecolor='black')\n",
        "    plt.title(\"Final Evaluation Metrics (LSTM)\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "    # Forecast next 120 days\n",
        "    # Use the full dataset for training\n",
        "    X_full = data_scaled\n",
        "    X_full, _ = create_sequences(X_full, look_back)\n",
        "    X_full = X_full.reshape(X_full.shape[0], X_full.shape[1], 1)\n",
        "\n",
        "    # Train on the full dataset\n",
        "    model.fit(X_full, data_scaled[look_back:], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Make prediction for 120 days\n",
        "    forecast = []\n",
        "    input_seq = X_full[-1]  # Take the last sequence from the training data\n",
        "\n",
        "    for _ in range(forecast_steps):\n",
        "        prediction = model.predict(input_seq.reshape(1, look_back, 1))\n",
        "        forecast.append(prediction[0, 0])\n",
        "        input_seq = np.append(input_seq[1:], prediction, axis=0)\n",
        "\n",
        "    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
        "\n",
        "    # Plot forecasted values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data, label=\"Original Data\")\n",
        "    plt.plot(np.arange(len(data), len(data) + forecast_steps), forecast, color='red', label=\"Forecast\")\n",
        "    plt.title(f\"120 Day Forecast Using LSTM\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics, forecast\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "metrics_lstm, forecast_120_days = lstm_cv_with_forecast(data_close, look_back=10, forecast_steps=120, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "Scj-7CU_KntS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Function to create sequences for GRU\n",
        "def create_sequences(data, look_back):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:i + look_back])\n",
        "        y.append(data[i + look_back])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Function to apply GRU with Cross Validation and future forecasting\n",
        "def gru_cv_with_forecast(data, look_back=10, n_splits=10, forecast_steps=120, epochs=10, batch_size=32):\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
        "\n",
        "    rmse_list = []\n",
        "    mae_list = []\n",
        "    mape_list = []\n",
        "    r2_list = []\n",
        "\n",
        "    for train_index, test_index in tscv.split(data_scaled):\n",
        "        train, test = data_scaled[train_index], data_scaled[test_index]\n",
        "        X_train, y_train = create_sequences(train, look_back)\n",
        "        X_test, y_test = create_sequences(test, look_back)\n",
        "\n",
        "        # Reshape data for GRU [samples, time steps, features]\n",
        "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "        # Define GRU Model\n",
        "        model = Sequential([\n",
        "            GRU(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "            GRU(50, return_sequences=False),\n",
        "            Dropout(0.2),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        # Train Model\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Predict\n",
        "        predictions = model.predict(X_test)\n",
        "        predictions = scaler.inverse_transform(predictions)\n",
        "        y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "        # Calculate Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        mape = np.mean(np.abs((y_test - predictions) / y_test)) * 100\n",
        "        r2 = r2_score(y_test, predictions)\n",
        "\n",
        "        # Append to Lists\n",
        "        rmse_list.append(rmse)\n",
        "        mae_list.append(mae)\n",
        "        mape_list.append(mape)\n",
        "        r2_list.append(r2)\n",
        "\n",
        "    # Calculate Average Metrics\n",
        "    avg_rmse = np.mean(rmse_list)\n",
        "    avg_mae = np.mean(mae_list)\n",
        "    avg_mape = np.mean(mape_list)\n",
        "    avg_r2 = np.mean(r2_list)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"GRU Model Evaluation:\")\n",
        "    print(f\"Average RMSE: {avg_rmse}\")\n",
        "    print(f\"Average MAE: {avg_mae}\")\n",
        "    print(f\"Average MAPE: {avg_mape}%\")\n",
        "    print(f\"Average R^2: {avg_r2}\")\n",
        "\n",
        "    # Forecast next 120 days\n",
        "    # Use the full dataset for training\n",
        "    X_full = data_scaled\n",
        "    X_full, _ = create_sequences(X_full, look_back)\n",
        "    X_full = X_full.reshape(X_full.shape[0], X_full.shape[1], 1)\n",
        "\n",
        "    # Train on the full dataset\n",
        "    model.fit(X_full, data_scaled[look_back:], epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Make prediction for 120 days\n",
        "    forecast = []\n",
        "    input_seq = X_full[-1]  # Take the last sequence from the training data\n",
        "\n",
        "    for _ in range(forecast_steps):\n",
        "        prediction = model.predict(input_seq.reshape(1, look_back, 1))\n",
        "        forecast.append(prediction[0, 0])\n",
        "        input_seq = np.append(input_seq[1:], prediction, axis=0)\n",
        "\n",
        "    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
        "\n",
        "    # Plot forecasted values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(np.arange(len(data), len(data) + forecast_steps), forecast, color='red', label=\"Forecast\")\n",
        "    plt.title(f\"120 Day Forecast Using GRU\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return forecast\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame containing the 'Close' column\n",
        "data_close = data['Close'].values  # Replace with your dataset's 'Close' column\n",
        "forecast_120_days = gru_cv_with_forecast(data_close, look_back=10, forecast_steps=120, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "MmSS6qC5L77J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function for Prophet Forecasting\n",
        "def prophet_forecast(data, forecast_days=120):\n",
        "    # Prepare data for Prophet\n",
        "    df = pd.DataFrame({'ds': data.index, 'y': data['Close']})\n",
        "\n",
        "    # Initialize and fit the model\n",
        "    model = Prophet()\n",
        "    model.fit(df)\n",
        "\n",
        "    # Create future dataframe\n",
        "    future = model.make_future_dataframe(periods=forecast_days)\n",
        "\n",
        "    # Make forecast\n",
        "    forecast = model.predict(future)\n",
        "\n",
        "    # Plot full forecast including historical data\n",
        "    fig1 = model.plot(forecast)\n",
        "    plt.title(\"Prophet Forecast (Including Historical Data)\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot forecasted values separately\n",
        "    forecast_only = forecast[-forecast_days:]  # Extract only future predictions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(forecast_only['ds'], forecast_only['yhat'], label='Forecasted Prices', color='red')\n",
        "    plt.title(f\"Forecasted Prices for Next {forecast_days} Days (Prophet)\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return forecast_only\n",
        "\n",
        "# Example Usage\n",
        "# Assuming 'data' is your DataFrame with a 'Close' column and a DateTime index\n",
        "\n",
        "forecast_prophet = prophet_forecast(data, forecast_days=120)\n"
      ],
      "metadata": {
        "id": "vW7qnODIPh40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Transformer Model using Keras\n",
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder = layers.Dense(d_model, activation='relu')\n",
        "        self.transformer = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.decoder = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.encoder(inputs)\n",
        "        attention_output = self.transformer(x, x)\n",
        "        x = self.flatten(attention_output)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Prepare Data and Train Transformer\n",
        "def transformer_forecast(data, look_back=30, forecast_days=120):\n",
        "    # Preprocess data\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "    def create_sequences(data, look_back):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - look_back):\n",
        "            X.append(data[i:i + look_back])\n",
        "            y.append(data[i + look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X, y = create_sequences(data_scaled, look_back)\n",
        "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
        "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
        "\n",
        "    # Reshape input data for the model\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Shape: (samples, time_steps, features)\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # Define Model\n",
        "    model = TransformerModel(input_dim=1, d_model=64, nhead=4, num_layers=2, output_dim=1)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train Model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "    # Forecast\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "    # Plot Results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Close'][-forecast_days:].values, label='True Prices')\n",
        "    plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"Transformer: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example Usage\n",
        "transformer_predictions = transformer_forecast(data, look_back=30, forecast_days=120)\n"
      ],
      "metadata": {
        "id": "AaYWS0_BMhsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Autoformer Model using Keras\n",
        "class Autoformer(tf.keras.Model):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
        "        super(Autoformer, self).__init__()\n",
        "        self.encoder = layers.Dense(d_model, activation='relu')\n",
        "        self.decomposition = layers.LSTM(d_model, return_sequences=True)\n",
        "        self.transformer = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.decoder = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.encoder(inputs)\n",
        "        x = self.decomposition(x)\n",
        "        attention_output = self.transformer(x, x)\n",
        "        x = self.flatten(attention_output)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Prepare Data and Train Autoformer\n",
        "def autoformer_forecast(data, look_back=30, forecast_days=120):\n",
        "    # Preprocess data\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "    def create_sequences(data, look_back):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - look_back):\n",
        "            X.append(data[i:i + look_back])\n",
        "            y.append(data[i + look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X, y = create_sequences(data_scaled, look_back)\n",
        "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
        "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
        "\n",
        "    # Reshape input data for the model\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Shape: (samples, time_steps, features)\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # Define Model\n",
        "    model = Autoformer(input_dim=1, d_model=64, nhead=4, num_layers=2, output_dim=1)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train Model\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "    # Forecast\n",
        "    predictions = model.predict(X_test)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "    # Plot Results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Close'][-forecast_days:].values, label='True Prices')\n",
        "    plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"Autoformer: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example Usage\n",
        "autoformer_predictions = autoformer_forecast(data, look_back=30, forecast_days=120)\n"
      ],
      "metadata": {
        "id": "JdkrDfXsgV0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rp-OXxRCjA2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# ارزیابی مدل با معیارهای مختلف\n",
        "def evaluate_model(true_values, predicted_values):\n",
        "    # محاسبه RMSE (Root Mean Squared Error)\n",
        "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
        "\n",
        "    # محاسبه MAE (Mean Absolute Error)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "\n",
        "    # محاسبه R2 (R-Squared)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    # محاسبه RME (Relative Mean Error)\n",
        "    rme = np.mean(np.abs((true_values - predicted_values) / true_values)) * 100\n",
        "\n",
        "    # چاپ نتایج\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R2: {r2:.4f}\")\n",
        "    print(f\"RME: {rme:.4f}%\")\n",
        "\n",
        "    return rmse, mae, r2, rme\n"
      ],
      "metadata": {
        "id": "0W_iedJVioJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# بعد از پیش‌بینی در مدل Autoformer\n",
        "autoformer_predictions = autoformer_forecast(data, look_back=30, forecast_days=120)\n",
        "\n",
        "# ارزیابی پیش‌بینی‌ها\n",
        "evaluate_model(data['Close'][-120:].values, autoformer_predictions)\n"
      ],
      "metadata": {
        "id": "yrBfXwwKiqq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Informer(tf.keras.Model):\n",
        "    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim):\n",
        "        super(Informer, self).__init__()\n",
        "        self.encoder = layers.Dense(d_model, activation='relu')\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.decoder = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Encoder -> Attention -> Decoder\n",
        "        x = self.encoder(inputs)\n",
        "        attn_output = self.attn(x, x)\n",
        "        output = self.decoder(attn_output)\n",
        "        return output\n",
        "\n",
        "def informer_forecast(data, look_back=30, forecast_days=120):\n",
        "    # Prepare data (no need for scaling now)\n",
        "    def create_sequences(data, look_back):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - look_back):\n",
        "            X.append(data[i:i + look_back])\n",
        "            y.append(data[i + look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    # Extract closing prices and create sequences\n",
        "    data_close = data['Close'].values\n",
        "    X, y = create_sequences(data_close, look_back)\n",
        "\n",
        "    # Split into train and test data\n",
        "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
        "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
        "\n",
        "    # Reshape to be 3D: [batch_size, seq_len, features]\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # (batch_size, seq_len, features)\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))  # (batch_size, seq_len, features)\n",
        "\n",
        "    # Define model\n",
        "    model = Informer(input_dim=1, d_model=64, num_heads=4, num_layers=2, output_dim=1)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Flatten predictions to match the shape of the true values\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Close'][-forecast_days:].values, label='True Prices')\n",
        "    plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"Informer: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example Usage\n",
        "informer_predictions = informer_forecast(data, look_back=30, forecast_days=120)\n"
      ],
      "metadata": {
        "id": "bTPZvqU3kiz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJ8-HrSqkwJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTM_Model(tf.keras.Model):\n",
        "    def __init__(self, units, output_dim):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "        self.lstm = layers.LSTM(units, return_sequences=False)\n",
        "        self.dense = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.lstm(inputs)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "\n",
        "# Prepare data for training and testing\n",
        "def prepare_data(data, look_back=30, forecast_days=120):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "    def create_sequences(data, look_back):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - look_back):\n",
        "            X.append(data[i:i + look_back])\n",
        "            y.append(data[i + look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X, y = create_sequences(data_scaled, look_back)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
        "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n",
        "# Define training and prediction function\n",
        "def lstm_forecast(data, look_back=30, forecast_days=120):\n",
        "    X_train, y_train, X_test, y_test, scaler = prepare_data(data, look_back, forecast_days)\n",
        "\n",
        "    # Define model\n",
        "    model = LSTM_Model(units=64, output_dim=1)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_scaled = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions back to the original scale\n",
        "    predictions = scaler.inverse_transform(predictions_scaled)\n",
        "\n",
        "    # Flatten predictions\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Close'][-forecast_days:].values, label='True Prices')\n",
        "    plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"LSTM: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example Usage\n",
        "informer_predictions = lstm_forecast(data, look_back=30, forecast_days=120)\n"
      ],
      "metadata": {
        "id": "BIw1hh3RsZyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ارزیابی پیش‌بینی‌ها\n",
        "evaluate_model(data['Close'][-120:].values, informer_predictions)"
      ],
      "metadata": {
        "id": "ICAOk9f2vEBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTM_Model(tf.keras.Model):\n",
        "    def __init__(self, units, output_dim):\n",
        "        super(LSTM_Model, self).__init__()\n",
        "        self.lstm = layers.LSTM(units, return_sequences=False)\n",
        "        self.dense = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.lstm(inputs)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "\n",
        "# Prepare data for training and testing\n",
        "def prepare_data(data, look_back=30, forecast_days=120):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data_scaled = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "    def create_sequences(data, look_back):\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - look_back):\n",
        "            X.append(data[i:i + look_back])\n",
        "            y.append(data[i + look_back])\n",
        "        return np.array(X), np.array(y)\n",
        "\n",
        "    X, y = create_sequences(data_scaled, look_back)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    X_train, y_train = X[:-forecast_days], y[:-forecast_days]\n",
        "    X_test, y_test = X[-forecast_days:], y[-forecast_days:]\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler\n",
        "\n",
        "# Define training and prediction function\n",
        "def lstm_forecast(data, look_back=30, forecast_days=120):\n",
        "    X_train, y_train, X_test, y_test, scaler = prepare_data(data, look_back, forecast_days)\n",
        "\n",
        "    # Define model\n",
        "    model = LSTM_Model(units=64, output_dim=1)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_scaled = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform predictions back to the original scale\n",
        "    predictions = scaler.inverse_transform(predictions_scaled)\n",
        "\n",
        "    # Flatten predictions\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "    # Plot True Prices vs Predicted Prices\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Close'][-forecast_days:].values, label='True Prices', color='blue')\n",
        "    #plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"LSTM: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "  # Plot True Prices vs Predicted Prices\n",
        "    plt.figure(figsize=(10, 6))\n",
        "   # plt.plot(data['Close'][-forecast_days:].values, label='True Prices', color='blue')\n",
        "    plt.plot(predictions, label='Predicted Prices', color='red')\n",
        "    plt.title(\"LSTM: Predicted vs Actual Prices\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Evaluate model performance\n",
        "def evaluate_model(true_values, predicted_values):\n",
        "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "    r2 = r2_score(true_values, predicted_values)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "    print(f\"R-squared (R2): {r2}\")\n",
        "\n",
        "    # Plot the evaluation metrics\n",
        "    metrics = [rmse, mae, r2]\n",
        "    metrics_names = ['RMSE', 'MAE', 'R²']\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(metrics_names, metrics, color=['blue', 'green', 'orange'])\n",
        "    plt.title(\"Model Evaluation Metrics\")\n",
        "    plt.ylabel(\"Values\")\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "# Assuming data is already loaded and preprocessed\n",
        "informer_predictions = lstm_forecast(data, look_back=30, forecast_days=120)\n",
        "\n",
        "# Evaluate predictions\n",
        "evaluate_model(data['Close'][-120:].values, informer_predictions)\n"
      ],
      "metadata": {
        "id": "q_xsE7kDvx-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}